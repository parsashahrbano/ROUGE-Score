This repository contains a Python script designed to evaluate the quality of automatic summaries by comparing them to corresponding gold standard summaries. The script works by first identifying folders that contain gold standard summaries, with each folder named after a specific set of documents. It then matches these folders with automatically generated summaries stored in a separate directory, using the folder name to find the corresponding auto summary. The gold standard summary within each folder is named gold.txt, and the matching auto summary is identified by its file name. Using the Rouge score, the script evaluates the similarity between each auto summary and its respective gold summary, calculating F-scores for Rouge-1, Rouge-2, and Rouge-L metrics. The results are saved in a CSV file, providing a concise view of the summary quality for further analysis. This setup can handle multiple gold summaries, making it suitable for datasets like DUC2004, where each cluster has multiple reference summaries.
